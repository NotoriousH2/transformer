{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660403b7",
   "metadata": {},
   "source": [
    "# Huggingface에서 쉽게 트랜스포머 모델 써보기\n",
    "안녕하세요! 강사 변형호입니다.   \n",
    "이 실습에서는 지난 시간에 2주에 걸쳐 열심히 배웠던 트랜스포머 기반의 모델을 코드 한 줄로 사용해 보겠습니다.\n",
    "\n",
    "**허깅페이스**[https://www.huggingface.co/] 는 트랜스포머와 같은 인공지능 모델을 쉽게 사용할 수 있는 서비스입니다.  \n",
    "개발자가 자신의 모델을 자유롭게 업로드하고 공유할 수 있는 오픈 소스 플랫폼이고, 트랜스포머뿐만 아니라 최근에 사용되는 거의 모든 인공지능 모델들은 다 여기서 확인할 수 있다고 봐도 되는데요.     \n",
    "이번에는 트랜스포머 모델을 가져와서 간단한 실습을 실행해 보겠습니다.\n",
    "\n",
    "* 이 실습 파일은 https://huggingface.co/learn/nlp-course/ko/chapter1/3?fw=pt 를 참고하여 만들었습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e0746",
   "metadata": {},
   "source": [
    "\n",
    "저희의 실습 환경에서는 따로 설치가 되어 있지 않기 때문에 아래의 코드를 실행하여 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8d4ddd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.0 fsspec-2023.10.0 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7ff56",
   "metadata": {},
   "source": [
    "설치 후에는 import를 통해 불러옵니다.  \n",
    "pipeline은 중간 과정을 생략하고 이미 만들어진 모델을 통해 원하는 문제를 빠르게 실행해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c4fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117b8b5",
   "metadata": {},
   "source": [
    "파이프라인에 문제를 설정하고 문장을 입력해 주면 기본 모델에 의한 결과를 돌려줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc95536",
   "metadata": {},
   "source": [
    "아래의 문제들을 실행해 보겠습니다.   \n",
    "\n",
    "sentiment-analysis : 감정 분석   \n",
    "text-generation : 텍스트 생성   \n",
    "fill-mask : 마스크 채우기   \n",
    "question-answering : 질의 응답   \n",
    "summarization : 요약   \n",
    "\n",
    "zero-shot-classification : 제로샷 분류(규칙을 스스로 찾아서 학습하는 문제)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644ef25c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998764991760254}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 분석 : 문장을 입력받으면 긍정/부정을 분류하여 출력\n",
    "classifier = pipeline(\"sentiment-analysis\") \n",
    "classifier(\"I had a really delightful meal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812799b",
   "metadata": {},
   "source": [
    "I had a really delightful meal. 이라는 문장은 99.99%로 긍정적인 문장이라고 분류했습니다.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3873ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My professor does not like my grades. I don\\'t really understand how my grades make sense.\"\\n\\nHe added: \"But there are certain things you can do to earn a good GPA – like writing a good job, doing things that help people'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 생성 : 문장을 입력받아 뒷부분을 추론하여 출력 (성능은 별로...)\n",
    "generator = pipeline('text-generation')\n",
    "generator('My professor does not like my')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd4c3c",
   "metadata": {},
   "source": [
    "문장의 뒷 내용을 추론하여 채워 줍니다.   \n",
    "텍스트 생성의 기본 모델은 gpt2입니다. (붉은 warning message에도 나와 있습니다.) <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188b19a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6801381707191467,\n",
       "  'token': 3529,\n",
       "  'token_str': ' eat',\n",
       "  'sequence': 'We have to eat balanced meal'},\n",
       " {'score': 0.030325133353471756,\n",
       "  'token': 2254,\n",
       "  'token_str': ' enjoy',\n",
       "  'sequence': 'We have to enjoy balanced meal'},\n",
       " {'score': 0.023026229813694954,\n",
       "  'token': 2807,\n",
       "  'token_str': ' choose',\n",
       "  'sequence': 'We have to choose balanced meal'},\n",
       " {'score': 0.021803976967930794,\n",
       "  'token': 1306,\n",
       "  'token_str': ' ensure',\n",
       "  'sequence': 'We have to ensure balanced meal'},\n",
       " {'score': 0.02157534658908844,\n",
       "  'token': 146,\n",
       "  'token_str': ' make',\n",
       "  'sequence': 'We have to make balanced meal'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마스크 채우기 : 빈칸에 무슨 단어가 들어갈 지 추론하기\n",
    "unmasker = pipeline('fill-mask')\n",
    "unmasker('We have to <mask> balanced meal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303de241",
   "metadata": {},
   "source": [
    "mask로 표시된 부분에 맞는 후보 단어를 찾아 점수 순으로 출력합니다.  <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83a5bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9106036424636841, 'start': 14, 'end': 18, 'answer': 'math'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질의응답 : 질문에 맞는 답변하기 (성능은 별로...)\n",
    "# Context와 Question을 넣어서 상황과 질문을 전달\n",
    "\n",
    "qa = pipeline('question-answering')\n",
    "qa(question='Which class did I fail?',\n",
    "  context='I got an F in math class because I have missed too many classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcc21c",
   "metadata": {},
   "source": [
    "상황과 질문을 주고 간단한 질의응답을 수행합니다.    \n",
    "수학 수업을 너무 많이 빼먹어서 F를 받았다는 상황에서, 정확한 답변을 하고 있습니다.<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f36f45c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4945891cc0f64b86bd2c255954cd595a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c783559ba2834d41a4f2557eaf480477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0932ac54c35441b8a1b3e3dbd70e381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35593bdd3f1648b5b4a117a388c566ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb735a3791842dc9aeee1eb7ed486b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 06:55:45.906442: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf888a45e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-29 06:55:45.908687: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-10-29 06:55:46.277462: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-29 06:55:47.194941: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Transformers is a 2007 american science fiction action film based on Hasbro's toy line of the same name . it stars Shia LaBeouf as Sam Witwicky, a teenager who gets caught up in a war between the heroic Autobots and the villainous Decepticons . the autobots intend to retrieve and use the AllSpark to rebuild their home planet Cybertron and end the war .\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 : 긴 글 요약하기\n",
    "\n",
    "summarizer = pipeline(\"summarization\")  # 요약\n",
    "summarizer(\"\"\"\n",
    "Transformers is a 2007 American science fiction action film based on Hasbro's toy line of the same name. \n",
    "It is the first installment in the Transformers film series. \n",
    "The film is directed by Michael Bay from a screenplay by Roberto Orci and Alex Kurtzman. \n",
    "It stars Shia LaBeouf as Sam Witwicky, a teenager who gets caught up in a war between the heroic Autobots and the villainous Decepticons, two factions of alien robots who can disguise themselves by transforming themselves into everyday machinery, primarily vehicles. \n",
    "The Autobots intend to retrieve and use the AllSpark, the powerful artifact that created their robotic race that is on Earth, to rebuild their home planet Cybertron and end the war, while the Decepticons have the intention of using it to build an army by giving life to the machines of Earth. \n",
    "Tyrese Gibson, Josh Duhamel, Anthony Anderson, Megan Fox, Rachael Taylor, John Turturro and Jon Voight also star, while Peter Cullen and Hugo Weaving voice Optimus Prime and Megatron, respectively.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375015f",
   "metadata": {},
   "source": [
    "영화 트랜스포머에 대한 한 문단의 글을 잘 요약했습니다. <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9e6c1d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-ko-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Transpomer became the basis for the modern AI model.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 번역 (한국어 --> 영어)\n",
    "translator2 = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ko-en\")  \n",
    "translator2(\"트랜스포머는 현대 AI 모델의 기반이 되었습니다.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe04025",
   "metadata": {},
   "source": [
    "번역의 경우 한국어 문장도 학습이 되어 있어야 하므로 다른 모델[https://huggingface.co/Helsinki-NLP/opus-mt-ko-en] 을 명시하여 불러왔습니다.   \n",
    "transformers의 pipeline을 이용하면 이렇게 특정 모델을 다운로드하여 바로 활용할 수 있습니다. <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be00406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I eat banana!',\n",
       " 'labels': ['Elemantary level',\n",
       "  'Middle school level',\n",
       "  'High school level',\n",
       "  'College/university level'],\n",
       " 'scores': [0.34958964586257935,\n",
       "  0.22910840809345245,\n",
       "  0.22269058227539062,\n",
       "  0.198611319065094]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제로 샷 분류 : 아무런 사전정보 없이 분류시키기\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
    "zero_shot_classifier(\n",
    "    \"I eat banana!\",\n",
    "    candidate_labels=[\"Elemantary level\", \"Middle school level\", \"High school level\", 'College/university level'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1456ac",
   "metadata": {},
   "source": [
    "또한, zero-shot-classification (제로샷 분류)를 수행할 수 있는데요.   \n",
    "이것은 사전에 저장된 방법이 아니라, 단순히 문장과 분류 후보를 주면    \n",
    "트랜스포머가 추론을 통해 답을 찾는 방식입니다.   \n",
    "기본값으로 큰 모델을 불러오기 때문에 시간이 조금 소요됩니다.    \n",
    "\n",
    "제가 낸 문제는 '이 말을 하는 사람이 초등/중/고등/대 학생 중에 어떤 사람일까?' 를 맞추는 문제였는데요.    \n",
    "예제에서는 대략 맞췄지만 정확히 이해하지는 못한 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bc884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350d52a2",
   "metadata": {},
   "source": [
    "이렇게 기존 모델을 불러와서 사용하는 것도 물론 가능하지만,    \n",
    "실제 환경에서는 사용자가 쓰고 싶은 데이터에 맞게 트랜스포머 모델의 가중치를 변화시키는 방법을 사용합니다.    \n",
    "이를 파인 튜닝(Fine-tuning)이라고 하는데요,    \n",
    "저희가 이전에 fasttext로 ko100.bin을 학습시켜서 얻은 임베딩에 건축 관련 데이터를 추가했던 것과 유사합니다.   \n",
    "\n",
    "단, 파인 튜닝은 모델이 클수록 시간이 오래 걸리므로 최신의 GPT-3.5와 같은 거대 언어 모델에서는 다른 방법들도 함께 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd2207",
   "metadata": {},
   "source": [
    "다음 실습 시간에는 트랜스포머와 GPT를 사용하여 실제 챗봇을 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40573bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
